{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ovizero01/Machine-Leaning/blob/main/022.5_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run below code cell to load dataset to colab"
      ],
      "metadata": {
        "id": "HJnz0uuk73KJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "RMXTSHB97_fB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.1\n",
        "### Train an AdaBoostClassifier with n_estimators=1 (just one tree).\n",
        "\n",
        "## Task 1.2\n",
        "### Train an AdaBoostClassifier with n_estimators=50 (multiple trees).\n",
        "\n",
        "## Task 1.3\n",
        "### Compare both of them , which one is better."
      ],
      "metadata": {
        "id": "HxY0NW2F8QvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.1\n",
        "base_tree = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "ada1 = AdaBoostClassifier(\n",
        "    estimator=base_tree,\n",
        "    n_estimators=1,\n",
        "    random_state=42\n",
        ")\n",
        "ada1.fit(X_train, y_train)\n",
        "y_pred1 = ada1.predict(X_test)\n",
        "print(\"One Tree Accuracy:\", accuracy_score(y_test, y_pred1))"
      ],
      "metadata": {
        "id": "6WqCxCff8qjy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b2bbb56-b62a-44c0-f8d7-793e2438f959"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One Tree Accuracy: 0.8947368421052632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.2\n",
        "ada50 = AdaBoostClassifier(\n",
        "    estimator=base_tree,\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "ada50.fit(X_train, y_train)\n",
        "y_pred50 = ada50.predict(X_test)\n",
        "print(\"50 Trees Accuracy:\", accuracy_score(y_test, y_pred50))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaaIObga1S1M",
        "outputId": "483c8585-be64-4c51-dc42-31f99c483927"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50 Trees Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.3\n",
        "The AdaBoost model with 50 trees is better because combining many weak learners improves accuracy, while 1 tree is too weak to perform well."
      ],
      "metadata": {
        "id": "n-wRH1KQ3CP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.1\n",
        "### Train a model with Gradient Boost with both low and high learning rate.\n",
        "\n",
        "## Task 2.2\n",
        "### Compare  \"Fast Learner\" (high learning rate) vs. a \"Slow Learner\" (low learning rate)."
      ],
      "metadata": {
        "id": "iL0cw9NW9Drx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2.1\n",
        "gbr_slow = GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "gbr_slow.fit(X_train, y_train)\n",
        "y_pred_slow = gbr_slow.predict(X_test)\n",
        "\n",
        "gbr_fast = GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.5,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "gbr_fast.fit(X_train, y_train)\n",
        "y_pred_fast = gbr_fast.predict(X_test)"
      ],
      "metadata": {
        "id": "SUX7oZFn9aEZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.2\n",
        "The fast learner(high learning rate) learns quickly but can overfit and be unstable, while the slow learner(low learning rate) learns gradually and is usually more accurate and stable on test data."
      ],
      "metadata": {
        "id": "_XeZVTnJCdIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3.1\n",
        "### Train a model with XGBoost with both shallow and deep tree\n",
        "\n",
        "## Task 3.2\n",
        "### Compare a \"Shallow\" tree (depth=2) vs. a \"Deep\" tree (depth=15)."
      ],
      "metadata": {
        "id": "j7ZKe4YO-Hz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3.1\n",
        "xgb_shallow = XGBClassifier(\n",
        "    max_depth=2,\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    objective=\"binary:logistic\",\n",
        "    eval_metric=\"logloss\",\n",
        "    random_state=42\n",
        ")\n",
        "xgb_shallow.fit(X_train, y_train)\n",
        "y_pred_shallow = xgb_shallow.predict(X_test)\n",
        "\n",
        "xgb_deep = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=15,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    objective=\"binary:logistic\",\n",
        "    eval_metric=\"logloss\",\n",
        "    random_state=42,\n",
        ")\n",
        "xgb_deep.fit(X_train, y_train)\n",
        "y_pred_deep = xgb_deep.predict(X_test)"
      ],
      "metadata": {
        "id": "nMoYaI9V-TLa"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.2\n",
        "A shallow tree(depth=2) is simple, may underfit, but generalizes better on unseen data, while a deep tree(depth=15) can capture complex patterns and achieve high training accuracy but is prone to overfitting and less stable on test data."
      ],
      "metadata": {
        "id": "FYcb9_lGEebJ"
      }
    }
  ]
}
